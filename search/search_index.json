{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Orthweb - Orthanc Solution on AWS","text":""},{"location":"#overview","title":"Overview","text":"<p>The Orthweb project automates the creation of a cloud-based mini-PACS based on Orthanc and Amazon Web Services (AWS). The project artifact addresses the cloud foundation and configuration management, and enables adopters to host the Orthanc software as a service (SaaS). To get started, follow the documentation. \ud83d\udcaa Let's automate medical imaging!</p> <p>Imaging systems handling sensitive data must operate on secure platforms. Typically, large organizations dedicate specialized IT resources to build their enterprise-scale cloud foundations. This cloud foundation, also known as a landing zone, addresses security and scalability. Each line of business of the large organization is allocated with a segment (e.g. an VPC) in the landing zone, to deploy their own applications.</p> <p>However, many Orthanc adopters are small teams without overarching cloud strategies from their parent organizations. They are startups, research groups, independent clinics, and so on. To leverage Orthanc capabilities, they need simple cloud foundations that are equally secure and scalable. To close this gap, we proposed and implemnted a cloud-based Orthanc solution: the Orthweb project. </p> <p></p> <p>To build the foundation fast, Orthweb project uses Terraform template (an infrastructure-as-code technology) to provision a self-contained infrastrcture stack in a single AWS account, without relying upon established network infrastructure. The infrastructure layer provisioned in this project contains a single VPC with multiple subnets, along with optional VPC endpoints. The infrastructure layer also contains encryption keys, managed database service and S3 storage.</p> <p>The Orthweb project also streamlines the configuration of Orthanc solution, by proposing a paradign for Orthanc configuration management. The project leverages cloud-init user data and <code>makefile</code> to configure the servers during the server's initialization process. The artifact to install Orthanc is stored in a separate repository for adopters to fork and customize. The orthanc-config repository is a good example. </p> <p>The project orchestrates the application containers with Docker daemon on EC2 instances. Technical users can expect to build a cloud-based mini-PACS in one hour with rich feature, scalability and security. For those considering hosting Orthanc on Kubernetes, check out the sister project Korthweb.</p>"},{"location":"deployment/configuration/","title":"Configuration","text":"<p>When an EC2 instance is launched, the cloud-init process executes a script as defined in user-data on the first boot. This initializes the configuration management.</p>"},{"location":"deployment/configuration/#pattern","title":"Pattern","text":"<p>The main steps of the user data script is illustrated below. </p> <pre><code>graph LR\n  subgraph OuterRectangle[\"EC2 Instance\"]\n    subgraph MiddleRectangle[\"Cloud Init\"]\n      InnerRectangle1[\"1 git clone\"]\n      InnerRectangle2[\"2 update parameter\"]\n      InnerRectangle3[\"3 execute command\"]\n    end\n  end\n  InnerRectangle1 --&gt; |deployment_options&lt;br&gt;ConfigRepo| Repo[\"orthanc-config repository\"]\n  InnerRectangle3 --&gt; |deployment_options&lt;br&gt;InitCommand| Command[\"cd orthanc-config&lt;br&gt; &amp;&amp; make aws\"]</code></pre> <p>First, the script pulls the configuration repo as specified to a local directory. Then it updates parameter file in the repo. Last, the scripts executes the command given in the variable, from the directory. They are configured in the Terraform variable <code>deployment_options</code>.</p>"},{"location":"deployment/configuration/#bootstrapping","title":"Bootstrapping","text":"<p>The configuration management repo is orthanc-config. Review the <code>README.md</code> file in the repo for details. In short, the repo suits multiple environments. For example, on AWS EC2, the initial command is to <code>make aws</code>. Under the hood, the <code>makefile</code> is the key to orchestrate the configuration activities in shell commands. Such activities include:</p> <ul> <li>Updating configuration file for Orthanc (e.g. S3 and database connectivity string)</li> <li>Checking dependencies on EC2 instance</li> <li>Building self-signed certificates</li> <li>Initializing database in RDS for Keycloak</li> </ul> <p>The <code>makefile</code> takes environment variables set from <code>.env</code> file, which looks like this:</p> <p><pre><code># Wherever applicable configuration files should be used as a primary means of configuration\nSITE_NAME=orthweb.digihunch.com\nORG_NAME=Hunch Digial Services Inc\n\n# NGINX PROXY\nNGINX_DOCKER_IMAGE=nginx:1.27.3\nSITE_KEY_CERT_FILE=config/certs/${SITE_NAME}.pem\n\n# ORTHANC SERVICE\nVERBOSE_ENABLED=true\nTRACE_ENABLED=true\nORTHANC_DOCKER_IMAGE=orthancteam/orthanc:25.1.1\nORTHANC_DB_HOST=orthanc-db\nORTHANC_DB_NAME=orthancdb\nORTHANC_DB_USERNAME=orthancdbusr\nORTHANC_DB_PASSWORD=orthancdbpsw\nORTHANC_KC_REALM=orthanc\nORTHANC_KC_CLIENT_ID=orthanc\n\n# ORTHANC AUTHORIZATION SERVICE\nAUTH_SERVICE_DOCKER_IMAGE=digihunchinc/orthanc-auth-service:1.0.0\nAUTH_SVC_UN=share-user\nAUTH_SVC_PSW=share-password\n\n# KEYCLOAK\nKEYCLOAK_DOCKER_IMAGE=orthancteam/orthanc-keycloak:25.1.0\nKC_DB_HOST=keycloak-db\nKC_DB_NAME=keycloakdb\nKC_DB_USERNAME=keycloakdbusr\nKC_DB_PASSWORD=keycloakdbpsw\nKC_ADMIN_USR=admin\nKC_ADMIN_PSW=changeme\n\n# POSTGRESQL\nPOSTGRESQL_DOCKER_IMAGE=postgres:17.2\n</code></pre> Some entries in this file are configured based on the Terraform templates input variable.</p>"},{"location":"deployment/configuration/#makefile","title":"Makefile","text":"<p>Here is what the <code>makefile</code> looks like to implement those activities:</p> <pre><code>.PHONY: TBD\n# For consistent command interpreter behaviour across Linux and Mac, set shell to bash. Otherwise, command substitution breaks\nSHELL := /bin/bash \ninclude .env\nCERT_COUNTRY=CA\nCERT_STATE=Ontario\nCERT_LOC=Toronto\nCERT_ORG=DigiHunch\nCERT_OU=Imaging\nCERT_DAYS=1095\nAUTH_SERVICE_INTERNAL_SECRET_KEY:=$(shell LC_ALL=C tr -dc 'A-Za-z0-9' &lt; /dev/urandom | head -c 16)\n\ndev: dep certs local done \naws: dep dep_ec2 certs psql ec2 done\n\ndep:\n    $(info --- Checking Dependencies for dev deployment ---)\n    @if ! command -v docker &amp;&gt; /dev/null; then echo \"Docker is not installed. Please install Docker.\"; exit 1; fi\n    @if ! command -v yq &amp;&gt; /dev/null; then echo \"yq is not installed. Please install yq.\"; exit 1; fi\n    @echo [makefile][$@] dependency check passed!\n    $(eval SANS := DNS:$(SITE_NAME),DNS:issuer.$(SITE_NAME))\n    @echo [makefile][$@] SANs is set to $(SANS)\ndep_ec2:\n    $(info --- Checking Dependencies for non-dev deployment ---)\n    @if ! command -v jq &amp;&gt; /dev/null; then echo \"jq is not installed. Please install jq.\"; exit 1; fi\n    @if ! command -v psql &amp;&gt; /dev/null; then echo \"postgresql is not installed. Please install postgresql.\"; exit 1; fi\n    @echo [makefile][$@] dependency check on ec2 passed!\n    $(eval TOKEN := $(shell curl -s -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"))\n    $(eval ServerPublicHostName := $(shell curl -s -H \"X-aws-ec2-metadata-token: $(TOKEN)\" http://169.254.169.254/latest/meta-data/public-hostname))\n    $(eval SANS := $(SANS),DNS:$(ServerPublicHostName))\n    @echo [makefile][$@] SANs is updated to $(SANS) \ncerts:\n    $(info --- Creating Self-signed Certificates ---)\n    @echo [makefile][$@] starting to create self-signed certificate for $(SITE_NAME) in config/certs\n    @openssl req -x509 -sha256 -newkey rsa:4096 -days $(CERT_DAYS) -nodes -subj /C=$(CERT_COUNTRY)/ST=$(CERT_STATE)/L=$(CERT_LOC)/O=$(CERT_ORG)/OU=$(CERT_OU)/CN=issuer.$(SITE_NAME)/emailAddress=info@$(SITE_NAME) -keyout config/certs/ca.key -out config/certs/ca.crt 2&gt;/dev/null\n    @openssl req -new -newkey rsa:4096 -nodes -subj /C=$(CERT_COUNTRY)/ST=$(CERT_STATE)/L=$(CERT_LOC)/O=$(CERT_ORG)/OU=$(CERT_OU)/CN=$(SITE_NAME)/emailAddress=issuer@$(SITE_NAME) -addext extendedKeyUsage=serverAuth -addext subjectAltName=$(SANS) -keyout config/certs/server.key -out config/certs/server.csr 2&gt;/dev/null\n    @openssl x509 -req -sha256 -days $(CERT_DAYS) -in config/certs/server.csr -CA config/certs/ca.crt -CAkey config/certs/ca.key -set_serial 01 -out config/certs/server.crt -extfile &lt;(echo subjectAltName=$(SANS))\n    @cat config/certs/server.key config/certs/server.crt config/certs/ca.crt &gt; config/certs/$(SITE_NAME).pem\n    @openssl req -new -newkey rsa:4096 -nodes -subj /C=$(CERT_COUNTRY)/ST=$(CERT_STATE)/L=$(CERT_LOC)/O=$(CERT_ORG)/OU=$(CERT_OU)/CN=client.$(SITE_NAME)/emailAddress=client@$(SITE_NAME) -keyout config/certs/client.key -out config/certs/client.csr 2&gt;/dev/null\n    @openssl x509 -req -sha256 -days $(CERT_DAYS) -in config/certs/client.csr -CA config/certs/ca.crt -CAkey config/certs/ca.key -set_serial 01 -out config/certs/client.crt\n    @echo [makefile][$@] finished creating self-signed certificate\nlocal:\n    $(info --- Configuring Orthanc for local dev ---)\n    @cp config/orthanc/orthanc.json.local config/orthanc/orthanc.json\n    @yq e '.services.orthanc-auth-service.environment.SECRET_KEY = \"$(AUTH_SERVICE_INTERNAL_SECRET_KEY)\"' docker-compose.yaml.local &gt; docker-compose.yaml\npsql:\n    $(info --- Provisioning PostgreSQL database for Keycloak on RDS ---)\n    @sed s/keycloak_db/$(KC_DB_NAME)/g config/keycloak_db/keycloak-provision.sql.tmpl &gt; config/keycloak_db/keycloak-provision.sql\n    @export PGPASSWORD=$(KC_DB_PASSWORD); psql \"host=$(KC_DB_HOST) port=5432 user=$(KC_DB_USERNAME) dbname=postgres sslmode=require\" -f config/keycloak_db/keycloak-provision.sql\n    @echo [makefile][$@] initialized postgresdb for keycloak\nec2:\n    $(info --- Configuring Orthanc on EC2 with S3 and RDS storage ---) \n    @jq '.AwsS3Storage = {ConnectionTimeout: 30, RequestTimeout: 1200, RootPath: \"image_archive\", StorageStructure: \"legacy\", BucketName: \"$(S3_BUCKET)\", Region: \"$(S3_REGION)\"} | del(.StorageDirectory) | .PostgreSQL.EnableSsl = true | .Plugins += [\"/usr/share/orthanc/plugins-available/libOrthancAwsS3Storage.so\"] ' config/orthanc/orthanc.json.local &gt; config/orthanc/orthanc.json\n    @yq e '.services.orthanc-auth-service.environment.SECRET_KEY = \"$(AUTH_SERVICE_INTERNAL_SECRET_KEY)\" | del(.services.keycloak-db, .services.orthanc-db) | .services.orthanc-service.depends_on |= map(select(. != \"orthanc-db\")) | .services.keycloak-service.depends_on |= map(select(. != \"keycloak-db\")) | .services.keycloak-service.environment.KC_DB_URL += \"?ssl=true&amp;sslmode=require\" ' docker-compose.yaml.local &gt; docker-compose.yaml\n    @echo [makefile][$@] updated configuration on ec2 \ndone:\n    $(info --- Configuration completed ---)\n    @echo [makefile][$@] Bootstrapping process:\n    @echo [makefile][$@]   1. Run docker compose up and grab the value for KEYCLOAK_CLIENT_SECRET from stdout\n    @echo [makefile][$@]   2. Edit compose file, update value for KEYCLOAK_CLIENT_SECRET, and set ENABLE_KEYCLOAK_API_KEYS to true\n    @echo [makefile][$@]   3. Restart the application: docker compose down then docker compose up\n</code></pre> <p>By default, we use the cloud-init script invoke the make command.To watch for the log output and errors from during cloud-init, check the cloud init log file (/var/log/cloud-init-output.log) on the EC2 instance. For convenience, the output from the command also prints the next steps to finish the configuration.</p> <p>A critical output of the <code>makefile</code> driven automation is the <code>docker-compose.yaml</code> file from the template, which allows user to start the system with docker compose command from the working directory. </p>"},{"location":"deployment/infrastructure/","title":"Infrastructure","text":""},{"location":"deployment/infrastructure/#overview","title":"Overview","text":"<p>Since we execute Terraform commands locally to drive the deployment, we also store Terraform states locally. Advanced Terraform users may choose to managed Terraform platform such as HCP Terraform(previously known as Terraform Cloud) which is beyond the scope of this document. </p> <p>Now we can start deploying Orthanc. From your command terminal, go to the <code>terraform</code> directory, and run <code>terraform</code> commands from this directory.</p>"},{"location":"deployment/infrastructure/#terraform-init","title":"Terraform Init","text":"<p>First, initialize terraform template with this command: <pre><code>terraform init\n</code></pre> The command initializes Terraform template, including pulling external modules and download providers. Successful initialization should report the following line: </p> <pre><code>Terraform has been successfully initialized!\n</code></pre> <p>After initialization, terraform creates <code>.terraform</code> directory to store the pulled modules and providers.</p>"},{"location":"deployment/infrastructure/#adjust-variables","title":"Adjust Variables","text":"<p>There are several ways to declare input variable in Terraform. In this solution, we use <code>terraform.tfvars</code> file in the terraform working directory. The file is loaded with functional input variables. Users should review the variables and adjust accordingly. Here is a sample of the <code>terraform.tfvars</code> file:</p> <pre><code>ec2_config = {\n  InstanceType  = \"t3.medium\"\n  PublicKeyData = null\n  PublicKeyPath = \"~/.ssh/id_rsa.pub\"\n}\nnetwork_config = {\n  vpc_cidr              = \"172.17.0.0/16\"\n  dcm_cli_cidrs         = [\"0.0.0.0/0\"]\n  web_cli_cidrs         = [\"0.0.0.0/0\"]\n  az_count              = 2\n  public_subnet_pfxlen  = 24\n  private_subnet_pfxlen = 22\n  interface_endpoints   = []\n  vpn_client_cidr       = \"\" # 192.168.0.0/22\n  vpn_cert_cn_suffix    = \"vpn.digihunch.com\"\n  vpn_cert_valid_days   = 3650\n}\nprovider_tags = {\n  environment = \"dev\"\n  owner       = \"admin@digihunch.com\"\n}\ndeployment_options = {\n  ConfigRepo     = \"https://github.com/digihunchinc/orthanc-config.git\"\n  CWLogRetention = 3\n  EnableCWLog    = false\n  SiteName       = null\n  InitCommand    = \"pwd &amp;&amp; echo Custom Command &amp;&amp; make aws\"\n}\n</code></pre> <p>To determin the variable values, some decisions to make are:</p> <ul> <li>Value of provider tag and site name </li> <li>Size of EC2 instance and public key</li> <li>Number of availability zones, CIDR for the VPC and subnet sizing</li> <li>CIDR blocks of the web and dicom client to whitelist</li> <li>Whether or not to ship docker log to Cloud Watch and retention period</li> </ul> <p>In most cases, users at least need to update the provider tag and site name. Read the full document for input variables here.</p>"},{"location":"deployment/infrastructure/#terraform-plan","title":"Terraform Plan","text":"<p>We plan the deployment with command:</p> <pre><code>terraform plan\n</code></pre> <p>The is command projects the changes that will be applied to AWS. It will print out the resources and what changes Terraform will make.</p> <p>If you're running this command for the first time, Terraform will flag all resources as to be created. If you're running the command with a change of Terraform template, it will only mark the prospective changes. </p> <p>At the end of the result, it will summarize the actions to take, for example: <pre><code>Plan: 54 to add, 0 to change, 0 to destroy.\n</code></pre> If the plan fails, check the code and state file. The number of resources to add depends on the specific input variables and whether the solution has been previously deployed.</p>"},{"location":"deployment/infrastructure/#terraform-apply","title":"Terraform Apply","text":"<p>If the plan looks good, we can apply the deployment plan: <pre><code>terraform apply\n</code></pre> Then, you need to say <code>yes</code> to the prompt. Terraform kicks off the deployment.  </p> <p>During deployment, Terraform provider interacts with your AWS account to provision the resources. Some critical resources takes much longer than others due to sheer size. For example, the database alone takes 15 minutes. The EC2 instances also takes a few minutes because of the bootstrapping process that configures Orthanc application. The entire deployment process can take as long as 30 minutes. To fask track the progress, you parrallelize the deployment with flags such as <code>-parallelism=3</code>. </p>"},{"location":"deployment/infrastructure/#review-output","title":"Review Output","text":"<p>Upon successful deployment, the screen should print out four entries. They are explained in the table below:</p> key example value protocol purpose server_dns ec2-15-156-192-145.ca-central-1.compute.amazonaws.com, ec2-99-79-73-88.ca-central-1.compute.amazonaws.com (HTTPS and DICOM TLS) HTTPS/DICOM-TLS Business traffic: HTTPS on port 443 and DICOM-TLS on port 11112. Reachable from the Internet. host_info Primary:i-02d92d2c1c046ea62    Secondary:i-076b93808575da71e SSH For management traffic. s3_bucket wealthy-lemur-orthbucket.s3.amazonaws.com HTTPS-S3 For orthanc to store and fetch images. Access is restricted. db_endpoint wealthy-lemur-orthancpostgres.cqfpmkrutlau.us-east-1.rds.amazonaws.com:5432 TLS-POSTGRESQL For orthanc to index data. Access is restricted. <p>Once the screen prints the output, the EC2 instances may still take a couple extra minutes in the background to finish  configuring Orthanc. We can start validation as per the steps outlined in the next section. </p> <p>If applicable, deploy the custom application traffic management</p>"},{"location":"deployment/infrastructure/#terraform-state","title":"Terraform State","text":"<p>Terraform keeps a local file <code>terraform.tfstate</code> for the last known state of the deployed resources, known as the state file. This file is critical for the ongoing maintanance of the deployed resources.</p> <p>Ad hoc changes to the resources created by Terraform are not registered in the state file. These changes, often referred to as configuration drift, are very likely to cause issues when the Terraform managed resources are updated or deleted. In general, manual changes to Terraform managed resources should be avoided. Changes should be first registered in the Terraform template and applied via the <code>terraform apply</code> command.</p>"},{"location":"deployment/infrastructure/#cost-estimate","title":"Cost Estimate","text":"<p>Below is a per-day estimate of cost (in USD) of the infrastructure based on default configuration. </p> AWS Service Standing Cost Relational Database $3.6 EC2-Instances $2.2 VPC $0.6 Key Management Service $0.13 EC2-Other $0.12 Secrets Manager $0.12 S3 $0.13 Total daily cost $7 <p>Note, the numbers does not include data processing charges such as images stored to and retrieved from S3, or data moved in and out of the Internet Gateway, etc. The numbers also have free-tier usage factored in. Users are advised to run the solution with everyday usage to get a more realistic ballpark of daily cost. AWS has a comprehensive pricing calculator and saving plans available.</p>"},{"location":"deployment/infrastructure/#logs","title":"Logs","text":"<p>To view container logs from EC2 instance, use docker compose log command: <pre><code>docker compose logs -f\n</code></pre> If cloud watch log is enabled, the docker daemon configuration file is automatically configured on EC2 instances to ship logs to cloud watch log groups with the configured retention window. </p>"},{"location":"deployment/infrastructure/#clean-up","title":"Clean up","text":"<p>After the validation is completed, it is important to remember this step to stop incurring on-going cost.</p> <p>You can delete all the resources with <code>destroy</code> command: <pre><code>terraform destroy\n</code></pre> The command should report that all resources are deleted.</p>"},{"location":"deployment/preparation/","title":"Preparation","text":"<p>The solution was tested on Mac and Linux. The instructions are based on Mac or Linux. Use the solution on Windows at your own risk. There are also several ways to adjust the steps to work with managed Terraform environment (e.g. Terraform Cloud). For simplicity, this documentation assumes that you work from a command terminal.</p>"},{"location":"deployment/preparation/#prerequisite","title":"Prerequisite","text":"<p>In your command terminal, install the required packages:</p> <ul> <li>Make sure awscli is installed and configured so you can connect to your AWS account with as your IAM user (using <code>Access Key ID</code> and <code>Secret Access Key</code> with administrator privilege). If you will need to SSH to the EC2 instance, you also need to install session manager plugin;</li> <li>Make sure Terraform CLI is installed. In the Orthweb template, Terraform also uses your IAM credential to authenticate into AWS. </li> </ul> <p>Then, use Git to pull the repostory: <pre><code>git clone https://github.com/digihunch/orthweb.git\n</code></pre> Before running terraform command, enter the <code>orthweb</code> directory as current working directory.</p>"},{"location":"deployment/preparation/#additional-steps","title":"Additional Steps","text":"<p>Take the preparatory steps below if you need to inspect or troubleshoot the Orthanc deployment. Otherwise, skip to the next section to start Installation.</p>"},{"location":"deployment/preparation/#secure-ssh-access","title":"Secure SSH access","text":"<p>There are two ways to SSH to the EC2 instances. To use your own choice of command terminal, you must configure your RSA key pair on the EC2 instances. Alternatively, without your own RSA key pair, you may use web-based command terminal provided by Session Manager in AWS console.</p>"},{"location":"deployment/preparation/#use-your-own-command-terminal","title":"Use your own command terminal","text":"<p>You need to create your RSA key pair. Your public key will be stored as file <code>~/.ssh/id_rsa.pub</code> on MacOS or Linux by default. Here is how the template determines what to send to EC2 as authorized public key:</p> <ol> <li>If you specify public key data in the input variable <code>pubkey_data</code>, then it will added as authorized public key when the EC2 instances are created.</li> <li>If <code>pubkey_data</code> is not specified, then it looks for the file path specified in input variable <code>pubkey_path</code> for public key</li> <li>If <code>pubkey_path</code> is not specified, then it uses default public key path <code>~/.ssh/id_rsa.pub</code> and pass the public key</li> <li>If no file is found at the default public key path, then the template will not send a public key. The EC2 instances to be provisioned will not have an authorized public key. Your only option to SSH to the instance is using AWS web console.</li> </ol> <p>Terraform template picks up environment variable prefixed with <code>TF_VAR_</code> and pass them in as Terraform's input variable without the prefix in the name. For example, if you set environment as below before running <code>terraform init</code>, then Terraform will pick up the value for input variables <code>pubkey_data</code> and <code>pubkey_path</code>: <pre><code>export\nTF_VAR_pubkey_data=\"mockpublickeydatawhichissuperlongdonotputyourprivatekeyherepleaseabcxyzpubkklsss\"\nTF_VAR_pubkey_path=\"/tmp/mykey.pub\"\n</code></pre></p> <p>Your SSH client works in tandem with session-manager-plugin. You can add the following section to your local SSH configuration file (i.e. <code>~/.ssh/config</code>) so it allows the session manager proxies the SSH session for hostnames matching <code>i-*</code> and <code>mi-*</code>.</p> <p><pre><code>host i-* mi-*\n    ProxyCommand sh -c \"aws ssm start-session --target %h --document-name AWS-StartSSHSession --parameters 'portNumber=%p'\"\n    IdentityFile ~/.ssh/id_rsa\n    User ec2-user\n</code></pre> Then you will be able to directly ssh to an instance by its instance ID, even if the instance does not have a public IP. It will use Linux user <code>ec2-user</code>, whose public key has been pre-loaded authorized public key.</p>"},{"location":"deployment/preparation/#use-web-based-terminal","title":"Use web-based terminal","text":"<p>Orthweb automaically configures the permission required for EC2 instances to connect to AWS system manager.</p> <p>Log on to AWS console, from <code>AWS System Manager</code> in your region, on the left-hand pannel, under <code>Node Management</code>, select <code>Fleet Manager</code>. You should see your instances listed. Select the Node by the name, select <code>Node actions</code> and then <code>Start terminal session</code> (under <code>Connect</code>). It will take you to a web-based command console and logged in as <code>ssm-user</code>. You can switch to our <code>ec2-user</code> with sudo commands: <pre><code>sh-4.2$ sudo -s\n[root@ip-172-27-3-138 bin]# su - ec2-user\nLast login: Wed Nov 23 22:02:57 UTC 2022 from localhost on pts/0\n[ec2-user@ip-172-27-3-138 ~]$\n</code></pre></p> <p>Both mechanisms are enabled by default in the Terraform template.</p>"},{"location":"deployment/preparation/#custom-deployment-options","title":"Custom deployment options","text":"<p>This project comes with working default but you can customize it in certain ways, by modifying the variable file <code>terraform.tfvars</code>. The variables are self-explanatory and defined in <code>variables.tf</code> file. </p> variable description network_config Adjust the networking configuration (e.g. CIDRs, sizing) and specify interface endpoints to enable if required. provider_tags Adjust the resource tags to apply to every resources deployed through the Terraform template deployment_options Adjust the deployment specification. For example, use a different instance size, configuration repo, and have your own site name <p>If you use BYO DNS name, make sure to set the SiteName correctly. The value of site name, if set, is used in several configuration files for Orthanc. If it is set incorrectly, you will not be able to browse the orthanc Site correctly. </p>"},{"location":"design/configmgmt/","title":"Configuration Management","text":"<p>Upon launching the EC2 instances, the Orthweb solution can pull from a public Git repo and automatically completes the configuration of Orthanc. Orthweb completes a baseline configuration for Orthanc. The configuration was based on the reference configuration repo for authorization service, with some simplification efforts.</p>"},{"location":"design/configmgmt/#automation","title":"Automation","text":"<p>The configuration files lives in a separate repo called orthanc-config, or otherwise specified. If you have custom configuration, you may fork this repository and reference it when deploying Orthweb.</p> <p>The repo directory can be placed in user directory <code>~</code> and it consists of a <code>makefile</code>. The file contains the steps and commands to automate the configuration for Orthanc. The <code>README.md</code> file in the repo has the instruction but the idea is to complete the installation with a single command. For example, on an EC2 instance, you may run <code>make aws</code>. This will call the required steps needed to configure Orthanc on the specific EC2 instance. On the other hand, if you simply need a generic development environment (e.g. on MacBook), you can run the <code>make dev</code> command. This command can even be supplied as a variable to the Terraform template such that it gets automatically executed.</p> <p>The repo consists of several directories. The key directory is <code>config</code>, where each subfolder is mapped to a directory in the file systems of running containers. </p>"},{"location":"design/configmgmt/#containers","title":"Containers","text":"<p>The orthanc applciation consists of the following container-based services. The containers either uses pre-built images by orthanc-team, or images built as needed in the Docker compose file. How these containers interact with each other is illustrated as below:</p> <pre><code>graph TD;\n    B[Web Browser] --&gt; |HTTPS&lt;br&gt;TCP 443|C{Nginx Proxy&lt;br&gt; Container};\n    X[DICOM Device] --&gt;|DICOM TLS&lt;br&gt;TCP 11112| C;\n    C --&gt;|HTTP&lt;br&gt;TCP 8042| D[Orthanc&lt;br&gt;Container];\n    C -. DICOM&lt;br&gt;TCP 4242 .-&gt; D;\n    D --&gt;|via Nginx| F;\n    C --&gt;|HTTP&lt;br&gt;TCP 8080| F[KeyCloak&lt;br&gt;Container];\n    D --&gt;|HTTP&lt;br&gt;TCP 8000| G[Custom Authorization&lt;br&gt;Service Container];\n    F --&gt;|via Nginx| G;</code></pre> <p>The Nginx proxy is configured to direct traffic to different containers based on path. For example, if the request path is <code>/keycloak/*</code>, then the HTTP request is directed to keycloak container. If the request path is <code>/orthanc/*</code>, then it request is directed to Orthanc container. The Orthanc container is able to connect to S3 and PostgreSQL database.</p>"},{"location":"design/configmgmt/#plugins","title":"Plugins","text":"<p>One of the key configuration file is <code>orthanc.json</code>, which defines the required configurations. Available configuraiton options can be found in Orthanc book. The official plugins in the baseline configuration include:</p> <ul> <li>AWS S3 Storage Plugin (for deployment on EC2 only)</li> <li>PostgreSQL Index Plugin </li> <li>Orthanc Explorer 2 Plugin</li> <li>Advanced Authorization Plugin</li> <li>DicomWeb Plugin</li> <li>Stone Web Viewer Plugin</li> </ul> <p>The Authorization plugin requires use of the Orthanc Explorer 2 plugin, and integrates with the external custom authorization service. The PostgreSQL database is to index the study data. The imaging pixel data are stored to S3 bucket using the S3 Storage Plugin.</p>"},{"location":"design/configmgmt/#application-logs","title":"Application logs","text":"<p>Container produces logs through docker deamon. To persist the log data, the solution provides a mechanism to tell Docker daemon to push container logs to Cloud Watch, with a configurable retention window. Even if an EC2 instance dies or containers fail, the log data are stored outside of the EC2 instance.</p>"},{"location":"design/deviceconnectivity/","title":"Device Connectivity","text":"<p>Device connectivity is another area of customization depending on the organization's network setup. </p> <p>Images with sensitive information are transferred in DICOM protocol. If they are transferred across the Internet, they must be encrypted with Transfer Layer Security (TLS). To configure secure DICOM transfer, both sides must support TLS. While this is configured by default in the orthanc solution, the modality side does not always have proper support for TLS.</p> <p>As a result, users should not send DICOM images from modalities over the Internet to Orthanc without TLS configuration. If the modality does not support DICOM TLS configuration, consider the following ways to secure the transfer.</p>"},{"location":"design/deviceconnectivity/#private-network-connection","title":"Private Network Connection","text":"<p>At network infrastructure level, the organzation may build a AWS Direct Connect connection with AWS. Requirement for such network connection should be reviewed with the network team of the organization, and require collaboration of multiple teams.</p> <p></p> <p>Instead of private physical connection, user may build a virtual private connection over the Internet using VPN.</p>"},{"location":"design/deviceconnectivity/#virtual-private-network-vpn","title":"Virtual Private Network (VPN)","text":"<p>Compared with Direct Connect, VPN involves less effort and cost. The solution can work with two models of VPN: </p> <ul> <li>Site-to-site VPN: requiring either a physical device or software application to act as a customer gateway. </li> <li>Client VPN: requiring OpenVPN-based client on one or more workstations. Enable split-tunnel so only relevent traffic are routed to VPC.</li> </ul> <p>If you're sending imaging data from a handful of workstations. Client VPN is a good approach, and it is implemented in the Orthweb as a module. The configuration is based on this instruction, which automates the following otherwise manual activities:</p> <ol> <li>the VPN client and the VPN endpoint use certificate based mutual authentication. Many use OpenSSL to create certificates but AWS instruction uses \"easyrsa3\" to create them. The template addon uses Terraform tls provider to create the certificates.</li> <li>When creating the VPN endpoint, specify a separate CIDR range for client IPs, e.g. <code>192.168.0.0/22</code> In this context, the client IP is the workstation's IP once it connects to the VPC via client VPN.</li> <li>create a new security group (e.g. vpn-ep-sg) with outbound rule allowing all types of traffic to destination CIDR 0.0.0.0/0</li> <li>When creating the VPN endpoint, associate it with the two private subnets as target network (which adds the required routes under the hood). Set vpn-ep-sg as the security group. Create the authorization rules as instructed.</li> <li>Enable split-tunnel for the VPN endpoint so other network features on the workstation are not impacted.</li> </ol> <p>Once the VPC client software (OpenVPN or AWS VPN client) is configured and connected from the workstation, the connection between the EC2 instance and the DICOM client will become secured at the IP layer. The application on the workstation connects to the server by private IP.</p>"},{"location":"design/deviceconnectivity/#use-a-dicom-proxy","title":"Use a DICOM proxy","text":"<p>The organization may consider running a local DICOM proxy. The proxy receives images from modality in the clear, and forwards the images over the Internet to Orthanc. Unlike the modality application, such proxy applications usually come with full support of TLS. There are not many open-source options. An on-prem instance of Orthanc can be configured to act as a DICOM proxy.</p> <p></p> <p>In this configuration the DICOM port should also open. Use security group to restrict where the port can receive traffic.</p>"},{"location":"design/infrastructure/","title":"Infrastructure","text":"<p>The Orthweb proposes a reference architecture to host Orthanc on AWS. The reference architecture does not include application traffic management. This section discusses how the components work in this architecture. The next page, will focus on the options for ingress traffic management.</p>"},{"location":"design/infrastructure/#the-reference-arthicture","title":"The Reference Arthicture","text":"<p>The architecure includes a VPC that spans across two availability zones (AZs). Each AZ has a public and a private subnet. Each public subnet stands one EC2 instance, with a public IP address routable from the Internet. The Reference Architecture is illustrated in the diagram below:</p> <p></p> <p>The two EC2 instances operates active-active, each with its own public IP. There are more options to manage application traffic and they are discussed separately in the section. The instances listens for DICOM (over TLS) and HTTPS traffic on TCP port <code>11112</code> and <code>443</code>. </p> <p>Within each EC2 instance, Docker daemon runs the required containers for Orthanc application. Orthweb uses Docker Compose to instruct Docker daemon how to orchestrate several containers of different purposes. The EC2 instances are launched using launch template.</p> <p>The private subnet contains the network interface for RDS PostgreSQL database. For the Orthanc container to connect to S3, an S3 gateway endpoint is created in each AZ. Optionally and at additional cost, users may enable interface endpoints in each AZ to route AWS management traffic privately.</p>"},{"location":"design/infrastructure/#redundancy","title":"Redundancy","text":"<p>The Orthweb solution provisions a pair of EC2 instances by default for redundancy. If you only need one of the two EC2 instance, feel free to stop the other one to avoid incurring charges. To stop and start an instance using AWS CLI, identify the instance ID and run: <pre><code>aws ec2 stop-instances --instance-ids i-12345678  # instance billing stops\naws ec2 start-instances --instance-ids i-12345678  # instance billing restarts\n</code></pre> Bear in mind that when there is only one EC2 instance running, it becomes a single point of failure.</p> <p>The pair of EC2 instances, in conjunction with additional cloud resources, can bring high availability to the solution. However, the Orthweb solution does not intend to address high availability with its out-of-the-box configuration. For high availability, options are discussed under application traffic management and usually require additional customization efforts.</p>"},{"location":"design/infrastructure/#certificate","title":"Certificate","text":"<p>TLS certificate (either self-signed or BYO certificates) must be applied to both HTTP and DICOM ports for traffic encryption, because the data contains patient information and travels across the Internet. Provisioning third-party certificate is the responsibility of the user, and the process to provision TLS certificate for an organization varies largely depending on the Public Key Infrastructure (PKI). </p> <p>The Orthweb solution provisions self-signed certificate in the configuration. It automatically configures Nginx proxy using the self-signed certificate for both TCP ports (443 and 11112). The limitation with Self-signed certificates is that they are not broadly trusted. However, they can still encrypt the traffic. The self-signed certificate is issued to the public DNS name of the EC2 instance as the certificate Common Name, in the format of <code>ec2-pub-lic-ip-addr.&lt;region&gt;.compute.amazonaws.com</code>. </p>"},{"location":"design/infrastructure/#network-paths","title":"Network Paths","text":"<p>The EC2 instances handles network traffic for both business and management purposes. Assuming the application traffic coming from the Internet arrives at the network interface of the two EC2 instances, we consider the following traffic pathes:</p> <ul> <li>DICOM and web traffic: connection from client browser or DICOM AE travels across Internet and arrives at EC2's network interface via the Internet Gateway of VPC. Returning taffic directed to the client goes through Internet Gateway. Both types of traffic are by default protected with TLS (transport layer security).</li> <li>Database traffic: only Orthanc container in each EC2 instance makes connection to RDS instance. The endpoint of RDS instance is deployed in the prviate subnets of the VPC. The database traffic does not leave the VPC. The traffic is protected with TLS.</li> <li>AWS Management traffic: by default, AWS management traffic such as secret manager, KMS, are routed through the Internet, and are encrypted with TLS. Optionally, users may introduce their interface endpoints in the VPC in order to route such traffic privately. </li> </ul> Choice of interface endpoints in VPC Routing Pattern Tradeoff <code>[]</code> (none by default) Without any interface endpoints, all types of AWS management traffic are routed through Internet. Most cost-efficient configuration. <code>[\"kms\",\"secretsmanager\"]</code> Traffic for critical management traffic (secrets and keys) is routed privately A balanced configration between security risk and cost <code>[\"kms\",\"secretsmanager\",\"ec2\",\"ssm\",\"ec2messages\",\"ssmmessages\"]</code> All types of AWS management traffic are routed privately. Most secure configuration but each interface endpoint incurs its own cost. <ul> <li>AWS Data Traffic: the EC2 instances makes connection to S3 to store imaging data. The terraform template creates S3 Gateway Endpoint such that the traffic from EC2 instance to S3 is routed privately. The traffic to S3 is protected with TLS encryption.</li> </ul>"},{"location":"design/infrastructure/#compliance","title":"Compliance","text":"<p>Below are some other concers in regard with the configurations for security compliance:</p> <ol> <li>Both DICOM and web traffic are encrypted in TLS. This requires peer DICOM AE to support DICOM TLS as well.</li> <li>PostgreSQL data is encrypted at rest, and the database traffic between Orthanc application and database is encrypted in SSL. The database endpoint is not open to public.</li> <li>The S3 bucket has server side encryption. The traffic in transit between S3 bucket and Orthanc application is encrypted as well.</li> <li>The password for database are generated dynamically and stored in AWS Secret Manager in AWS. The EC2 instance is granted access to the secret, which allows the configuration script to fetch the secret and launch container with it. </li> <li>The self-signed X509 certificate is dynamically generated using openssl11 during bootstrapping, in compliance with Mac requirement.</li> <li>Secret Manager, S3 and the KMS key used to encrypt objects and secrets all use resource-based IAM role to restrict access.</li> <li>VPC flow log and S3 access log are sent to a separate S3 bucket. However, the S3 access log usually takes 10 minutes to be delivered.</li> </ol>"},{"location":"design/infrastructure/#limitations","title":"Limitations","text":"<p>Currently there are also some limitation with secure configuration:</p> <ol> <li>Database secret rotation isn't implemented. Instead, Database password is generated at Terraform client and then sent to deployment server to create PostgreSQL. The generated password is also stored in state file of Terraform. To overcome this, the application would need to automatically receive secret update.</li> <li>Secret management with Docker container: secret are presented to container process as environment variables, instead of file content. This is generally secure enough but not the best practice, as per this article.</li> <li>Self-signed certificates are often flagged by the Browser as insecure. So users are advised to use their own certificates.</li> </ol>"},{"location":"design/ingress/","title":"Ingress Traffic","text":"<p>Ingress traffic management concerns with how external web traffic (and possibly DICOM traffic) reach the network interface of Orthanc EC2 instances. </p> <p>The Orthweb solution does not provide a prescriptive design pattern or implementation for ingress traffic management. This is because the requirements in this area often vary so significantly that no two organizations share the same design. </p> <p>This section discusses some possible customization options for ingress traffic management. Note, these patterns are not the only viable options. Please discuss the most suitable design with us.</p>"},{"location":"design/ingress/#out-of-box-configuration","title":"Out-of-box Configuration","text":"<p>The out-of-box configuration functions without ingress traffic management service. However, it comes with two difference DNS names, one for each EC2 instance, as illustrated below:</p> <p></p> <p>In this configuration, each EC2 instance lives in a separate availability zone. Both are connected to the same database (RDS) instance and storage (S3). In the event of an EC2 instance failure, the other instance is available. User may also choose to stop one of the instances for lower cost.</p> <p>On each EC2 instance, the Nginx container listens to port 443 and proxies web request according to its configuration.</p>"},{"location":"design/ingress/#limitations","title":"Limitations","text":"<p>In most production scenarios, the out-of-box configuration is not convinient. First, the two EC2 instances have two separate DNS names. In the event that one instance becomes unavailable, users and modalities have to use the alternative site DNS name. Second, the DNS name is automatically created based on the public IP address. Users do not have control of the DNS names. Thrid, the DNS names end with <code>amazonaws.com</code>, which is not owned by the users, and therefore users are not able to create trusted certificates.</p> <p>To bring the solution to produciton, it is recommended to introduce additional cloud resources to manage ingress traffic. The rest of the section discusses at very high level some options.</p>"},{"location":"design/ingress/#use-domain-naming-service-dns","title":"Use Domain Naming Service (DNS)","text":"<p>Consider introducing a DNS service to point to both EC2 instances. The DNS resolution result determins which EC2 instance the client connects to. So each EC2 instance must still open 443 and 11112 ports. This pattern is illustrated as below:</p> <p></p> <p>In this pattern, the DNS can resolves to the public DNS name for both EC2 instances. The result of DNS resolution can rotate, round robin or based on availability. In this option you will bring your own DNS name, and manage your own TLS certificate, instead of using the self-signed certificate provisioned during automation.</p> <p>It is also possible to integrate with Content Delivery Network (CDN, such as CloudFlare, CloudFront) for advanced features such as application firewall.</p>"},{"location":"design/ingress/#use-load-balancer-nlb-or-alb","title":"Use Load Balancer (NLB or ALB)","text":"<p>As cost allows, consider placing a network load balancer in front of the EC2 instances. We would be able to configure the network load balancer so it automatically sends the traffic to a functional EC2 instance, thereby eliminating the manual fail over procedure. This pattern is illustrated as below:</p> <p></p> <p>This configuration has several advantages. The security group of the EC2 instances can be narrowed down to only open to the load balancer. You can use Application Load Balancer or Network Load Balancer in AWS. The former supports integration with Web Application Firewall but only for HTTPS traffic. The latter supports both DICOM and HTTPS traffic. Both options supports integration with AWS Certificate Manager to automatically manage TLS certificate. </p>"},{"location":"design/ingress/#secure-application-traffic","title":"Secure Application Traffic","text":"<p>With appropriate Ingress Configuration the solution encrypts web traffic end-to-end. The DICOM image traffic technically can follow the same ingress traffic pattern. In some clinical settings, this is not always realistic, because many modalities do not support DICOM TLS sufficiently. The next section discusses some options to transfer DICOM images through different pathways. </p>"},{"location":"introduction/","title":"Introduction","text":""},{"location":"introduction/#overview","title":"Overview","text":"<p>Orthanc handles sensitive data and must be hosted on secure platforms. The motive of Orthweb project is to accelerate Orthanc deployment on Amazon Web Services (AWS) platform. The Orthweb project includes:</p> <ol> <li> <p>A prescriptive architecture optimized for hosting Orthanc. The architecture is opinionated but suitable for common scenarios. The architecture design is discussed in the Infrastructure section.</p> </li> <li> <p>The implementation artifact for the prescriptive architecture usingTerraform to manage infrastructure as code. The Orthanc Terraform template is available in the orthweb GitHub repository.</p> </li> <li> <p>A baseline Orthanc configuration with key plugins such as advanced authorization and auxiliary services such as KeyCloak. The artifact for configuration management is available in the orthanc-config GitHub repository. To customize configuration, create a fork of this repo.</p> </li> </ol> <p>While Orthweb provisions a fully functional Orthanc solution, there are some areas it does not intend to address. One example is ingress traffic management, whose design must account for integration with the current infrastructure and security setup, which is vastly different from organziation to organization. To drive this initiative in your organization, contact professional services at Digi Hunch\ud83d\udca1.</p>"},{"location":"introduction/#use-case","title":"Use case","text":"<p>If you have an AWS account without networking foundation for Orthanc, the template in Orthweb suits exactly to your needs. Then you may use the automation artifact in orthanc-config, or your own repository, to configure Orthanc. </p> <p>If you have pre-created networking layer (e.g. VPC, subnets), then you only need to create virtual machine with relevant dependencies before installing Orthanc. You can use Orthweb as a reference implementation to examine how the application interact with underlying cloud resources, and potentially reuse some Terraform modules in the repo. </p> <p>To allow Terraform to create resources in AWS, it needs sufficient permissions for deployment. Such permission for deployment usually requires administrator-level access.</p>"},{"location":"introduction/#choice-of-tools","title":"Choice of Tools","text":"<p>Orthweb is based on numerous open-source tools and commercial cloud services. Here are the rationales behind the choice:</p> <p>Terraform is a widely used infrastructure-as-code utility. The templates are written in Hashicorp Configuration Language(HCL), which strikes a good balance between declarativeness and level of abstraction. However, you do need to securely store Terraform state, and be wary of its workflow nuances. </p> <p>Docker is a simple way to host container workload. The Orthweb solution uses <code>Docker Compose</code> to orchestrate several containers of different purposes which is widely used by application developers. Amazon ECS is an alternative with some limitations and concerns on platform lock-in.</p> <p>PostgreSQL is the choice of database amongst all the database engines that Orthanc supports. It is feature rich and supports analytical workloads. AWS has two flavours of managed PostgreSQL: <code>RDS for PostgreSQL</code> and <code>Aurora PostgreSQL</code>. The Orthweb solutions works with the former. </p> <p>Nginx Proxy is a widely adopted reverse proxy to handle incoming requests. There are several alternative reverse proxy technologies. From 2022 to 2024, Orthweb uses Envoy Proxy. Since 2024, Orthweb switched back to Nginx as the default reverse proxy because it is the popular choice in the Orthanc user community.</p> <p>Amazon S3 is a scalable, feature-rich object storage platform well integrated with other AWS cloud services. S3 is the de-facto standard for object storage and Orthanc can store DICOM objects using S3 plugin. The pre-compiled binary for S3 plugin has been included the Orthanc release since 2022.</p>"},{"location":"introduction/#getting-started","title":"Getting Started","text":"<p>If you just want to start deploying Orthanc, skip right to the Deployment section. Otherwise, in the next section, we will discuss the architecture design, how to use and what to expect from the Orthweb solution.</p>"},{"location":"support/","title":"Support","text":"<p>For feature requests or bugs, please open an Issue on GitHub.</p> <p>Orthweb is dedicated to streamline the deployment experience. The idealistic intent of 1-click install often conflicts with the reality of having to integrate with a diverse range of incumbent configurations. To strke a balance, we share our best practices with a prescriptive architecture, and leave some aspects open to further customization. Please discuss with Digi Hunch\ud83d\udca1 about support and professional services. Some of our areas of expertise include:</p> <ul> <li>Custom Orthanc Configurations</li> <li>Custom Networking infrastructure</li> <li>Deployment of Orthanc in custom cloud platform</li> <li>Integration with existing services, such as security and IAM</li> <li>Design and build of custom cloud landing zone</li> <li>Improvement of Terraform workflow</li> </ul> <p>\u2764\ufe0f Thank you again for using Orthweb. \ud83d\udcaa Let's automate medical imaging!</p>"},{"location":"validation/additional/","title":"Additional Validation","text":""},{"location":"validation/additional/#overview","title":"Overview","text":"<p>In this section we connect to database and S3 storage for additional points of validation.</p>"},{"location":"validation/additional/#database-validation","title":"Database validation","text":"<p>The PostgreSQL RDS instance is accessible only from the EC2 instance on port 5432. You can get the database URL and credential from file <code>/home/ec2-user/.orthanc.env</code>. To validate by psql client, run:</p> <pre><code>sudo amazon-linux-extras enable postgresql14\nsudo yum install postgresql\npsql --host=postgresdbinstance.us-east-1.rds.amazonaws.com --port 5432 --username=myuser --dbname=orthancdb\n</code></pre> <p>Then you are in the PostgreSQL command console and can check the tables using SQL, for example:</p> <pre><code>orthancdb=&gt; \\dt;\n                List of relations\n Schema |         Name          | Type  | Owner\n--------+-----------------------+-------+--------\n public | attachedfiles         | table | myuser\n public | changes               | table | myuser\n public | deletedfiles          | table | myuser\n public | deletedresources      | table | myuser\n public | dicomidentifiers      | table | myuser\n public | exportedresources     | table | myuser\n public | globalintegers        | table | myuser\n public | globalproperties      | table | myuser\n public | maindicomtags         | table | myuser\n public | metadata              | table | myuser\n public | patientrecyclingorder | table | myuser\n public | remainingancestor     | table | myuser\n public | resources             | table | myuser\n public | serverproperties      | table | myuser\n(14 rows)\n\northancdb=&gt; select * from attachedfiles;\n id | filetype |                 uuid                 | compressedsize | uncompressedsize | compressiontype |         uncompressedhash         |          compressedhash          | revision\n----+----------+--------------------------------------+----------------+------------------+-----------------+----------------------------------+----------------------------------+----------\n  4 |        1 | 87719ef0-cbb1-4249-a0ac-e68356d97a7a |         525848 |           525848 |               1 | bd07bf5f2f1287da0f0038638002e9b1 | bd07bf5f2f1287da0f0038638002e9b1 |        0\n(1 row)\n</code></pre> <p>This is as far as we can go in terms of validating database. Without the schema document, we are not able to interpret the content. It is also not recommended to tamper with the tables directly bypassing the application.</p>"},{"location":"validation/additional/#storage-validation","title":"Storage Validation","text":"<p>Storage validation can be performed simply by examining the content of S3 bucket. Once studies are sent to Orthanc, the corresponding DICOM file should appear in the S3 bucket. For example, we can run the following AWS CLI command from the EC2 instance:</p> <pre><code>aws s3 ls s3://bucket-name\n2021-12-02 18:54:41     525848 87719ef0-cbb1-4249-a0ac-e68356d97a7a.dcm\n</code></pre> <p>The bucket is not publicly assissible and is protected by bucket policy configured during resource provisioning.</p>"},{"location":"validation/advanced/","title":"Advanced Validation","text":""},{"location":"validation/advanced/#overview","title":"Overview","text":"<p>In this section, we go through a few checkpoints through a system administrator's lens, to ensure the system is functional and correctly configured.</p>"},{"location":"validation/advanced/#server-validation","title":"Server Validation","text":"<p>Now we SSH to the server as <code>ec2-user</code>, as instructed above. Once connected, we can check cloud init log: <pre><code>sudo tail -F /var/log/cloud-init-output.log\n</code></pre> In the log, each container should say <code>Orthanc has started</code>. The configuration files related to Orthanc deployment are in directory <code>/home/ec2-user/orthanc-config</code>. Refer to the orthanc-config repository for how the configuration automation works. </p>"},{"location":"validation/advanced/#dicom-communication-tls","title":"DICOM communication (TLS)","text":"<p>To emulate DICOM activity,  we use dcmtk, with TLS options. We use the <code>echoscu</code> executable to issue <code>C-ECHO</code> DIMSE command, and the <code>storescu</code> executable to issue <code>C-STORE</code> commands. For example:</p> <p><pre><code>echoscu -aet TESTER -aec ORTHANC -d +tls client.key client.crt -rc +cf ca.crt ec2-35-183-66-248.ca-central-1.compute.amazonaws.com 11112\n</code></pre> The files <code>client.key</code>, <code>client.crt</code> and <code>ca.crt</code> can all be obtained from the /tmp/ directory on the server. </p> <p>The output should read Status code 0 in <code>C-ECHO-RSP</code>, followed by <code>C-ECHO-RQ</code>. Here is an example of the output from <code>storescu</code>:</p> <pre><code>I: Association Accepted (Max Send PDV: 16372)\nI: Sending Echo Request (MsgID 1)\nD: DcmDataset::read() TransferSyntax=\"Little Endian Implicit\"\nI: Received Echo Response (Success)\nI: Releasing Association\n</code></pre> <p>Further, we can store some DICOM part 10 file (usually .dcm extension containing images) to Orthanc server, using <code>storescu</code> executable:</p> <pre><code>storescu -aet TESTER -aec ORTHANC -d +tls client.key client.crt -rc +cf ca.crt ec2-35-183-66-248.ca-central-1.compute.amazonaws.com 11112 DICOM_Images/COVID/56364823.dcm\n</code></pre> <p>Below is an example of what the output from <code>storescu</code> should look like:</p> <pre><code>D: ===================== OUTGOING DIMSE MESSAGE ====================\nD: Message Type                  : C-STORE RQ\nD: Message ID                    : 427\nD: Affected SOP Class UID        : CTImageStorage\nD: Affected SOP Instance UID     : 1.3.6.1.4.1.9590.100.1.2.227776817313443872620744441692571990763\nD: Data Set                      : present\nD: Priority                      : medium\nD: ======================= END DIMSE MESSAGE =======================\nD: DcmDataset::read() TransferSyntax=\"Little Endian Implicit\"\nI: Received Store Response\nD: ===================== INCOMING DIMSE MESSAGE ====================\nD: Message Type                  : C-STORE RSP\nD: Presentation Context ID       : 41\nD: Message ID Being Responded To : 427\nD: Affected SOP Class UID        : CTImageStorage\nD: Affected SOP Instance UID     : 1.3.6.1.4.1.9590.100.1.2.227776817313443872620744441692571990763\nD: Data Set                      : none\nD: DIMSE Status                  : 0x0000: Success\nD: ======================= END DIMSE MESSAGE =======================\nI: Releasing Association\n</code></pre> <p>C-STORE-RSP status 0 indicates successful image transfer, and the image should viewable from the Orthanc site address. </p>"},{"location":"validation/advanced/#dicom-communication-without-tls","title":"DICOM communication (without TLS)","text":"<p>Caution: turn off TLS only if the images are transferred over private connection or encrypted connection. Refer to device connectivity for how to set up.</p> <p>To turn off TLS, locate the server configuration in the nginx configuration file for DICOM port, and remove the SSL options. For exmaple, here is what the snippet looks like with TLS encryption:  <pre><code>stream {\n    server {\n        listen                11112 ssl;\n        proxy_pass            orthanc-service:4242;\n        ssl_certificate       /usr/local/nginx/conf/site.pem;\n        ssl_certificate_key   /usr/local/nginx/conf/site.pem;\n        ssl_protocols         SSLv3 TLSv1 TLSv1.2 TLSv1.3;\n        ssl_ciphers           HIGH:!aNULL:!MD5:ECDH+AESGCM;\n        ssl_session_cache     shared:SSL:20m;\n        ssl_session_timeout   4h;\n        ssl_handshake_timeout 30s;\n    }\n}\n</code></pre> Here is what it looks like after removing TLS encryption:</p> <pre><code>stream {\n    server {\n        listen                11112;\n        proxy_pass            orthanc-service:4242;\n    }\n}\n</code></pre> <p>When using dcmtk utility for DICOM Ping or C-STORE, also remove the arguments related to tls.</p>"},{"location":"validation/basic/","title":"Basic Validation","text":""},{"location":"validation/basic/#overview","title":"Overview","text":"<p>We first perform a basic level of validation as an end user. Then we'll dive into technical validation with certain components. In all the validation steps, it is important to know the correct service address. If your environment comes with customized ingress configuration, such as using your domain name, content delivery network or load balancer, the service address used for testing will be different. </p> <p>The steps given are based on out-of-box configurations. So the service address looks like <code>ec2-35-183-66-248.ca-central-1.compute.amazonaws.com</code>.</p>"},{"location":"validation/basic/#dicom-ping","title":"DICOM ping","text":"<p>To Validate DICOM capability, we can test with C-ECHO and C-STORE. We can use any DICOM compliant application. For example, Horos on MacOS is a UI-based application. In Preference-&gt;Locations, configure a new DICOM nodes with:</p> <ul> <li>Address: the site address as given above</li> <li>AE title: ORTHANC</li> <li>Port: 11112 (or otherwise configured)</li> </ul> <p>Remember to enable TLS. Then you will be able to verify the node (i.e. C-ECHO) and send existing studies from Horos to Orthanc (C-STORE).</p>"},{"location":"validation/basic/#web-browser","title":"Web Browser","text":"<p>To Validate the the web service, simply visit the site address (with <code>https://</code> scheme) and put in the default credential at the prompt. Note that your web browser may flag the site as insecure because the server certificate's CA is self-signed and not trusted. </p> <p>Alternatively, you may use <code>curl</code> command to fetch the health check URI:</p> <p><pre><code>curl -HHost:web.orthweb.com -k -X GET https://ec2-35-183-66-248.ca-central-1.compute.amazonaws.com/nginx_health --cacert ca.crt\n</code></pre> The curl command should return 200 code.</p>"}]}